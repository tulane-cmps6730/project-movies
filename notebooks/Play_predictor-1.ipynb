{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gavingalusha/NLP/ggalusha/project/project-NBA/nlp-virtual/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freezing Parameters to deal with computation issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "\n",
    "# Load the configuration, model, and tokenizer\n",
    "configuration = GPT2Config.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', config=configuration)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Freeze parameters in the first few layers of the transformer\n",
    "for i, (name, param) in enumerate(model.transformer.h.named_parameters()):\n",
    "    # Example: Freeze the first 6 layers (you can adjust the range as needed)\n",
    "    if \"h.\" + str(i) in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Verify which layers are frozen\n",
    "for name, param in model.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(f\"Layer frozen: {name}\")  # This will print the names of parameters that are frozen\n",
    "\n",
    "# Continue with your training setup here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "class GPT2Dataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "\n",
    "        for text in texts:\n",
    "            encodings_dict = tokenizer('<|startoftext|>'+ text + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GameID</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>GameText</th>\n",
       "      <th>Label</th>\n",
       "      <th>CleanGameText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-10-27_CLE_CHI</td>\n",
       "      <td>1</td>\n",
       "      <td>695 N. Miroti misses 2-pt layup from 2 ft:694 ...</td>\n",
       "      <td>720 Jump ball: P. Gasol vs. T. Mozgov (D. Rose...</td>\n",
       "      <td>N. Miroti misses 2pt layup from  ft.  Defensiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-10-27_CLE_CHI</td>\n",
       "      <td>2</td>\n",
       "      <td>700 Defensive rebound by T. Gibson:685 D. Rose...</td>\n",
       "      <td>701 J. Cunningham misses 2-pt jump shot from 2...</td>\n",
       "      <td>Defensive rebound by T. Gibson.  D. Rose makes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-10-27_CLE_CHI</td>\n",
       "      <td>3</td>\n",
       "      <td>675 Turnover by D. Rose (bad pass; steal by L....</td>\n",
       "      <td>698 L. James makes 2-pt layup from 2 ft</td>\n",
       "      <td>Turnover by D. Rose bad pass.  steal by L. Jam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-10-27_CLE_CHI</td>\n",
       "      <td>4</td>\n",
       "      <td>695 Defensive rebound by T. Thompson:690 M. Wi...</td>\n",
       "      <td>696 E. Moore misses 2-pt jump shot from 16 ft</td>\n",
       "      <td>Defensive rebound by T. Thompson.  M. Williams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-10-27_DET_ATL</td>\n",
       "      <td>1</td>\n",
       "      <td>701 A. Drummond misses 2-pt layup from 1 ft (b...</td>\n",
       "      <td>720 Jump ball: A. Drummond vs. A. Horford (E. ...</td>\n",
       "      <td>A. Drummond misses 2pt layup from  ft block by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5353</th>\n",
       "      <td>2016-06-16_GSW_CLE</td>\n",
       "      <td>4</td>\n",
       "      <td>703 L. James makes free throw 2 of 2:703 L. Ja...</td>\n",
       "      <td>703 Shooting foul by H. Barnes (drawn by L. Ja...</td>\n",
       "      <td>L. James makes free throw  of .  L. James make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5354</th>\n",
       "      <td>2016-06-19_CLE_GSW</td>\n",
       "      <td>1</td>\n",
       "      <td>699 J. Smith misses 2-pt jump shot from 5 ft:6...</td>\n",
       "      <td>720 Jump ball: F. Ezeli vs. T. Thompson (L. Ja...</td>\n",
       "      <td>J. Smith misses 2pt jump shot from  ft.  Defen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5355</th>\n",
       "      <td>2016-06-19_CLE_GSW</td>\n",
       "      <td>2</td>\n",
       "      <td>686 R. Jefferson misses 2-pt jump shot from 5 ...</td>\n",
       "      <td>702 S. Livingston makes 2-pt jump shot from 16...</td>\n",
       "      <td>R. Jefferson misses 2pt jump shot from  ft blo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5356</th>\n",
       "      <td>2016-06-19_CLE_GSW</td>\n",
       "      <td>3</td>\n",
       "      <td>709 Defensive rebound by K. Love:698 J. Smith ...</td>\n",
       "      <td>709 F. Ezeli misses 2-pt jump shot from 3 ft</td>\n",
       "      <td>Defensive rebound by K. Love.  J. Smith makes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5357</th>\n",
       "      <td>2016-06-19_CLE_GSW</td>\n",
       "      <td>4</td>\n",
       "      <td>674 S. Livingston misses 2-pt jump shot from 1...</td>\n",
       "      <td>694 L. James makes 2-pt jump shot from 5 ft (a...</td>\n",
       "      <td>S. Livingston misses 2pt jump shot from  ft.  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5358 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  GameID  Quarter  \\\n",
       "0     2015-10-27_CLE_CHI        1   \n",
       "1     2015-10-27_CLE_CHI        2   \n",
       "2     2015-10-27_CLE_CHI        3   \n",
       "3     2015-10-27_CLE_CHI        4   \n",
       "4     2015-10-27_DET_ATL        1   \n",
       "...                  ...      ...   \n",
       "5353  2016-06-16_GSW_CLE        4   \n",
       "5354  2016-06-19_CLE_GSW        1   \n",
       "5355  2016-06-19_CLE_GSW        2   \n",
       "5356  2016-06-19_CLE_GSW        3   \n",
       "5357  2016-06-19_CLE_GSW        4   \n",
       "\n",
       "                                               GameText  \\\n",
       "0     695 N. Miroti misses 2-pt layup from 2 ft:694 ...   \n",
       "1     700 Defensive rebound by T. Gibson:685 D. Rose...   \n",
       "2     675 Turnover by D. Rose (bad pass; steal by L....   \n",
       "3     695 Defensive rebound by T. Thompson:690 M. Wi...   \n",
       "4     701 A. Drummond misses 2-pt layup from 1 ft (b...   \n",
       "...                                                 ...   \n",
       "5353  703 L. James makes free throw 2 of 2:703 L. Ja...   \n",
       "5354  699 J. Smith misses 2-pt jump shot from 5 ft:6...   \n",
       "5355  686 R. Jefferson misses 2-pt jump shot from 5 ...   \n",
       "5356  709 Defensive rebound by K. Love:698 J. Smith ...   \n",
       "5357  674 S. Livingston misses 2-pt jump shot from 1...   \n",
       "\n",
       "                                                  Label  \\\n",
       "0     720 Jump ball: P. Gasol vs. T. Mozgov (D. Rose...   \n",
       "1     701 J. Cunningham misses 2-pt jump shot from 2...   \n",
       "2               698 L. James makes 2-pt layup from 2 ft   \n",
       "3         696 E. Moore misses 2-pt jump shot from 16 ft   \n",
       "4     720 Jump ball: A. Drummond vs. A. Horford (E. ...   \n",
       "...                                                 ...   \n",
       "5353  703 Shooting foul by H. Barnes (drawn by L. Ja...   \n",
       "5354  720 Jump ball: F. Ezeli vs. T. Thompson (L. Ja...   \n",
       "5355  702 S. Livingston makes 2-pt jump shot from 16...   \n",
       "5356       709 F. Ezeli misses 2-pt jump shot from 3 ft   \n",
       "5357  694 L. James makes 2-pt jump shot from 5 ft (a...   \n",
       "\n",
       "                                          CleanGameText  \n",
       "0     N. Miroti misses 2pt layup from  ft.  Defensiv...  \n",
       "1     Defensive rebound by T. Gibson.  D. Rose makes...  \n",
       "2     Turnover by D. Rose bad pass.  steal by L. Jam...  \n",
       "3     Defensive rebound by T. Thompson.  M. Williams...  \n",
       "4     A. Drummond misses 2pt layup from  ft block by...  \n",
       "...                                                 ...  \n",
       "5353  L. James makes free throw  of .  L. James make...  \n",
       "5354  J. Smith misses 2pt jump shot from  ft.  Defen...  \n",
       "5355  R. Jefferson misses 2pt jump shot from  ft blo...  \n",
       "5356  Defensive rebound by K. Love.  J. Smith makes ...  \n",
       "5357  S. Livingston misses 2pt jump shot from  ft.  ...  \n",
       "\n",
       "[5358 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./Data/Actual_Quarter.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could add in a feature that Has all the player names in this game, could help with seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "# Ensure the tokenizer's pad_token_id is set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token\n",
    "\n",
    "# Initialize lists to store tokenized inputs\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "labels = []\n",
    "\n",
    "# Specify the number of instances you want to process\n",
    "num_instances = min(len(df), 500) \n",
    "\n",
    "# Process instances\n",
    "for idx in range(num_instances):\n",
    "    single_row = df.iloc[idx]\n",
    "    input_text = single_row['CleanGameText']\n",
    "    target_text = single_row['Label']\n",
    "\n",
    "    # Tokenize input and target texts. Note: No need for return_tensors='pt' here since we are appending to a list\n",
    "    input_tokens = tokenizer.encode(input_text, truncation=True, max_length=1024)\n",
    "    target_tokens = tokenizer.encode(target_text, truncation=True, max_length=1024)\n",
    "    \n",
    "    # Combine input and target tokens\n",
    "    combined_tokens = input_tokens + [tokenizer.eos_token_id] + target_tokens\n",
    "    \n",
    "    # Append combined tokens to input_ids list\n",
    "    input_ids.append(torch.tensor(input_tokens))\n",
    "    \n",
    "    # Create an attention mask for this sequence\n",
    "    attn_mask = [1] * len(input_ids)  # All tokens should be attended to\n",
    "    attention_masks.append(torch.tensor(attn_mask))\n",
    "    \n",
    "    # Use the same combined tokens as labels but with a shift\n",
    "    labels.append(torch.tensor(target_tokens))\n",
    "\n",
    "# Padding sequences and creating tensor datasets\n",
    "input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)  # -100 will be ignored by loss function\n",
    "\n",
    "# Move tensors to the appropriate device\n",
    "device = torch.device(\"cpu\")\n",
    "input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=2)  # Adjust the batch size as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take awhile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([1, 98])\n",
      "Attention Masks shape: torch.Size([1, 98])\n",
      "Labels shape: torch.Size([1, 98])\n"
     ]
    }
   ],
   "source": [
    "# Assuming input_text is your text data and target_text is the expected output for language modeling\n",
    "\n",
    "# Tokenize both input and output texts, ensuring the output is correctly aligned with the input\n",
    "input_encodings = tokenizer(input_text, truncation=True, padding='max_length', max_length=98, return_tensors=\"pt\")\n",
    "output_encodings = tokenizer(target_text, truncation=True, padding='max_length', max_length=98, return_tensors=\"pt\")\n",
    "\n",
    "# Prepare data\n",
    "input_ids = input_encodings.input_ids\n",
    "attention_masks = input_encodings.attention_mask\n",
    "labels = output_encodings.input_ids  # If the output tokens are expected as labels\n",
    "\n",
    "# Check shapes\n",
    "print(\"Input IDs shape:\", input_ids.shape)\n",
    "print(\"Attention Masks shape:\", attention_masks.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "# Assuming you're using these for creating a DataLoader\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_dataloader = DataLoader(dataset, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56968819e10c41df88fbbd21c0cdf265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete and model saved.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from transformers import AdamW, GPT2Model, GPT2Tokenizer\n",
    "\n",
    "# Assuming 'model' and 'tokenizer' are already initialized and loaded\n",
    "\n",
    "# Set the model to training mode and ensure it's on the correct device\n",
    "model.train()\n",
    "device = torch.device(\"cpu\")  # Change to 'cuda' if GPU is available\n",
    "model.to(device)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 3  # Adjust the number of epochs as necessary\n",
    "\n",
    "# Create a progress bar for the epochs\n",
    "progress_bar = tqdm(range(epochs), desc=\"Epochs\", leave=False)\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    for batch in train_dataloader:\n",
    "        # Unpack the batch data\n",
    "        input_ids, attention_masks, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the gradients on the optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: compute the model output\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass: compute the gradient of the loss w.r.t. the model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the progress bar with the latest loss\n",
    "        progress_bar.set_postfix(loss=loss.item(), refresh=True)\n",
    "\n",
    "    # Optionally save the model after each epoch\n",
    "    \n",
    "\n",
    "# Output the completion of training\n",
    "print(\"Training complete and model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel crashes when trying to do more than 100 ish rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to Early_Predictor\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"Early_Predictor\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input length of input_ids is 1520, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Generate text using the model\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 18\u001b[0m     test_output_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_input_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Setting a fixed max length for generation\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.95\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Enable sampling for more varied output\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Decode generated tokens back to text\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     generated_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(test_output_tokens[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/NLP/ggalusha/project/project-NBA/nlp-virtual/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NLP/ggalusha/project/project-NBA/nlp-virtual/lib/python3.10/site-packages/transformers/generation/utils.py:1449\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1443\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1444\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `generation_config` defines a `cache_implementation` that is not compatible with this model.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1445\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Make sure it has a `_setup_cache` function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1446\u001b[0m             )\n\u001b[1;32m   1447\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_cache(cache_cls, max_batch_size\u001b[38;5;241m=\u001b[39mbatch_size, max_cache_len\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[0;32m-> 1449\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_generated_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_default_max_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;66;03m# 7. determine generation mode\u001b[39;00m\n\u001b[1;32m   1452\u001b[0m generation_mode \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mget_generation_mode(assistant_model)\n",
      "File \u001b[0;32m~/NLP/ggalusha/project/project-NBA/nlp-virtual/lib/python3.10/site-packages/transformers/generation/utils.py:1140\u001b[0m, in \u001b[0;36mGenerationMixin._validate_generated_length\u001b[0;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_ids_length \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[1;32m   1139\u001b[0m     input_ids_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput length of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_ids_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but `max_length` is set to\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1142\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1144\u001b[0m     )\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;66;03m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m min_length_error_suffix \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincrease the maximum length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1150\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Input length of input_ids is 1520, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "num_test_instances = min(len(df), 5)  # Choose a number of instances to test, for example, 5\n",
    "\n",
    "for idx in range(num_test_instances):\n",
    "    test_row = df.iloc[idx]\n",
    "    test_input_text = test_row['CleanGameText']\n",
    "    actual_label_text = test_row['Label']  # The actual label (text) you want to predict\n",
    "\n",
    "    # Tokenize test input\n",
    "    test_input_tokens = tokenizer.encode(test_input_text, return_tensors='pt').to(device)\n",
    "\n",
    "    # Create attention mask\n",
    "    attention_mask = torch.ones(test_input_tokens.shape, dtype=torch.long, device=device)\n",
    "\n",
    "    # Generate text using the model\n",
    "    with torch.no_grad():\n",
    "        test_output_tokens = model.generate(\n",
    "            input_ids=test_input_tokens,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=50,  # Setting a fixed max length for generation\n",
    "            temperature=1.0,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True  # Enable sampling for more varied output\n",
    "        )\n",
    "\n",
    "        # Decode generated tokens back to text\n",
    "        generated_text = tokenizer.decode(test_output_tokens[0], skip_special_tokens=True)\n",
    "        input_length = len(tokenizer.decode(test_input_tokens[0], skip_special_tokens=True))\n",
    "\n",
    "        # Display the input, actual, and generated texts\n",
    "        print(f\"Instance {idx+1}\")\n",
    "        print(f\"Input text: {test_input_text}\")\n",
    "        print(f\"Generated text: {generated_text[input_length:]}\")  # Show generated continuation correctly\n",
    "        print(f\"Actual text: {actual_label_text}\")\n",
    "        print(\"-\" * 50)  # Separator for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [df.iloc[0].CleanGameText]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Freeze all layers except the last 2 transformer layers\n",
    "for name, param in model.named_parameters():\n",
    "    if 'h.' in name:  # Transformer blocks in GPT-2 are prefixed with 'h'\n",
    "        layer_number = int(name.split('.')[2])  # Extract the layer number\n",
    "        if layer_number < model.config.n_layer - 2:  # Freeze all but the last two layers\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Example data tokenization\n",
    "\n",
    "inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# TrainingArguments setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=inputs,  # Your training dataset\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
