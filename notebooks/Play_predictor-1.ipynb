{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freezing Parameters to deal with computation issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "\n",
    "# Load the configuration, model, and tokenizer\n",
    "configuration = GPT2Config.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2', config=configuration)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Freeze parameters in the first few layers of the transformer\n",
    "for i, (name, param) in enumerate(model.transformer.h.named_parameters()):\n",
    "    # Example: Freeze the first 6 layers (you can adjust the range as needed)\n",
    "    if \"h.\" + str(i) in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Verify which layers are frozen\n",
    "for name, param in model.named_parameters():\n",
    "    if not param.requires_grad:\n",
    "        print(f\"Layer frozen: {name}\")  # This will print the names of parameters that are frozen\n",
    "\n",
    "# Continue with your training setup here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "class GPT2Dataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "\n",
    "        for text in texts:\n",
    "            encodings_dict = tokenizer('<|startoftext|>'+ text + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GameID</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>GameText</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-10-27_CLE_CHI</td>\n",
       "      <td>1</td>\n",
       "      <td>720 Jump ball: P. Gasol vs. T. Mozgov (D. Rose...</td>\n",
       "      <td>669 Defensive rebound by T. Mozgov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-10-27_CLE_CHI</td>\n",
       "      <td>1</td>\n",
       "      <td>695 N. Miroti misses 2-pt layup from 2 ft:694 ...</td>\n",
       "      <td>663 M. Williams makes 2-pt jump shot from 21 ft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-10-27_CLE_CHI</td>\n",
       "      <td>1</td>\n",
       "      <td>694 Defensive rebound by K. Love:683 K. Love m...</td>\n",
       "      <td>649 P. Gasol misses 2-pt jump shot from 19 ft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-10-27_CLE_CHI</td>\n",
       "      <td>1</td>\n",
       "      <td>683 K. Love makes 2-pt dunk at rim (assist by ...</td>\n",
       "      <td>647 Defensive rebound by L. James</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-10-27_CLE_CHI</td>\n",
       "      <td>1</td>\n",
       "      <td>671 D. Rose misses 2-pt jump shot from 3 ft (b...</td>\n",
       "      <td>628 L. James makes 2-pt jump shot from 20 ft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574715</th>\n",
       "      <td>2016-06-19_CLE_GSW</td>\n",
       "      <td>4</td>\n",
       "      <td>11 L. James misses free throw 1 of 2:11 Clevel...</td>\n",
       "      <td>2 Offensive rebound by M. Speights</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574716</th>\n",
       "      <td>2016-06-19_CLE_GSW</td>\n",
       "      <td>4</td>\n",
       "      <td>11 Cleveland full timeout:11 Shooting foul by ...</td>\n",
       "      <td>1 M. Speights misses 3-pt jump shot from 24 ft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574717</th>\n",
       "      <td>2016-06-19_CLE_GSW</td>\n",
       "      <td>4</td>\n",
       "      <td>11 Shooting foul by D. Green (drawn by L. Jame...</td>\n",
       "      <td>0 Offensive rebound by Team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574718</th>\n",
       "      <td>2016-06-19_CLE_GSW</td>\n",
       "      <td>4</td>\n",
       "      <td>7 Personal foul by T. Thompson:2 S. Curry miss...</td>\n",
       "      <td>0 End of 4th quarter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574719</th>\n",
       "      <td>2016-06-19_CLE_GSW</td>\n",
       "      <td>4</td>\n",
       "      <td>2 S. Curry misses 3-pt jump shot from 26 ft:2 ...</td>\n",
       "      <td>0 End of Game</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>574720 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    GameID  Quarter  \\\n",
       "0       2015-10-27_CLE_CHI        1   \n",
       "1       2015-10-27_CLE_CHI        1   \n",
       "2       2015-10-27_CLE_CHI        1   \n",
       "3       2015-10-27_CLE_CHI        1   \n",
       "4       2015-10-27_CLE_CHI        1   \n",
       "...                    ...      ...   \n",
       "574715  2016-06-19_CLE_GSW        4   \n",
       "574716  2016-06-19_CLE_GSW        4   \n",
       "574717  2016-06-19_CLE_GSW        4   \n",
       "574718  2016-06-19_CLE_GSW        4   \n",
       "574719  2016-06-19_CLE_GSW        4   \n",
       "\n",
       "                                                 GameText  \\\n",
       "0       720 Jump ball: P. Gasol vs. T. Mozgov (D. Rose...   \n",
       "1       695 N. Miroti misses 2-pt layup from 2 ft:694 ...   \n",
       "2       694 Defensive rebound by K. Love:683 K. Love m...   \n",
       "3       683 K. Love makes 2-pt dunk at rim (assist by ...   \n",
       "4       671 D. Rose misses 2-pt jump shot from 3 ft (b...   \n",
       "...                                                   ...   \n",
       "574715  11 L. James misses free throw 1 of 2:11 Clevel...   \n",
       "574716  11 Cleveland full timeout:11 Shooting foul by ...   \n",
       "574717  11 Shooting foul by D. Green (drawn by L. Jame...   \n",
       "574718  7 Personal foul by T. Thompson:2 S. Curry miss...   \n",
       "574719  2 S. Curry misses 3-pt jump shot from 26 ft:2 ...   \n",
       "\n",
       "                                                  Label  \n",
       "0                    669 Defensive rebound by T. Mozgov  \n",
       "1       663 M. Williams makes 2-pt jump shot from 21 ft  \n",
       "2         649 P. Gasol misses 2-pt jump shot from 19 ft  \n",
       "3                     647 Defensive rebound by L. James  \n",
       "4          628 L. James makes 2-pt jump shot from 20 ft  \n",
       "...                                                 ...  \n",
       "574715               2 Offensive rebound by M. Speights  \n",
       "574716   1 M. Speights misses 3-pt jump shot from 24 ft  \n",
       "574717                      0 Offensive rebound by Team  \n",
       "574718                             0 End of 4th quarter  \n",
       "574719                                    0 End of Game  \n",
       "\n",
       "[574720 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./Data/Quarter.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could add in a feature that Has all the player names in this game, could help with seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"model = GPT2LMHeadModel.from_pretrained('gpt2')\\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "# Ensure the tokenizer's pad_token_id is set\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token\n",
    "\n",
    "# Initialize lists to store tokenized inputs\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "labels = []\n",
    "\n",
    "# Specify the number of instances you want to process\n",
    "num_instances = min(len(df), 700) \n",
    "\n",
    "# Process instances\n",
    "for idx in range(num_instances):\n",
    "    single_row = df.iloc[idx]\n",
    "    input_text = single_row['GameText']\n",
    "    target_text = single_row['Label']\n",
    "\n",
    "    # Tokenize input and target texts. Note: No need for return_tensors='pt' here since we are appending to a list\n",
    "    input_tokens = tokenizer.encode(input_text, truncation=True, max_length=1024)\n",
    "    target_tokens = tokenizer.encode(target_text, truncation=True, max_length=1024)\n",
    "    \n",
    "    # Combine input and target tokens\n",
    "    combined_tokens = input_tokens + [tokenizer.eos_token_id] + target_tokens\n",
    "    \n",
    "    # Append combined tokens to input_ids list\n",
    "    input_ids.append(torch.tensor(input_tokens))\n",
    "    \n",
    "    # Create an attention mask for this sequence\n",
    "    attn_mask = [1] * len(input_ids)  # All tokens should be attended to\n",
    "    attention_masks.append(torch.tensor(attn_mask))\n",
    "    \n",
    "    # Use the same combined tokens as labels but with a shift\n",
    "    labels.append(torch.tensor(target_tokens))\n",
    "\n",
    "# Padding sequences and creating tensor datasets\n",
    "input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "attention_masks = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=-100)  # -100 will be ignored by loss function\n",
    "\n",
    "# Move tensors to the appropriate device\n",
    "device = torch.device(\"cpu\")\n",
    "input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n",
    "\n",
    "# Create DataLoader\n",
    "train_data = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=2)  # Adjust the batch size as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will take awhile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([1, 98])\n",
      "Attention Masks shape: torch.Size([1, 98])\n",
      "Labels shape: torch.Size([1, 98])\n"
     ]
    }
   ],
   "source": [
    "# Assuming input_text is your text data and target_text is the expected output for language modeling\n",
    "\n",
    "# Tokenize both input and output texts, ensuring the output is correctly aligned with the input\n",
    "input_encodings = tokenizer(input_text, truncation=True, padding='max_length', max_length=98, return_tensors=\"pt\")\n",
    "output_encodings = tokenizer(target_text, truncation=True, padding='max_length', max_length=98, return_tensors=\"pt\")\n",
    "\n",
    "# Prepare data\n",
    "input_ids = input_encodings.input_ids\n",
    "attention_masks = input_encodings.attention_mask\n",
    "labels = output_encodings.input_ids  # If the output tokens are expected as labels\n",
    "\n",
    "# Check shapes\n",
    "print(\"Input IDs shape:\", input_ids.shape)\n",
    "print(\"Attention Masks shape:\", attention_masks.shape)\n",
    "print(\"Labels shape:\", labels.shape)\n",
    "\n",
    "# Assuming you're using these for creating a DataLoader\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "train_dataloader = DataLoader(dataset, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56968819e10c41df88fbbd21c0cdf265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete and model saved.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from transformers import AdamW, GPT2Model, GPT2Tokenizer\n",
    "\n",
    "# Assuming 'model' and 'tokenizer' are already initialized and loaded\n",
    "\n",
    "# Set the model to training mode and ensure it's on the correct device\n",
    "model.train()\n",
    "device = torch.device(\"cpu\")  # Change to 'cuda' if GPU is available\n",
    "model.to(device)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 3  # Adjust the number of epochs as necessary\n",
    "\n",
    "# Create a progress bar for the epochs\n",
    "progress_bar = tqdm(range(epochs), desc=\"Epochs\", leave=False)\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    for batch in train_dataloader:\n",
    "        # Unpack the batch data\n",
    "        input_ids, attention_masks, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_masks = attention_masks.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the gradients on the optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: compute the model output\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass: compute the gradient of the loss w.r.t. the model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the progress bar with the latest loss\n",
    "        progress_bar.set_postfix(loss=loss.item(), refresh=True)\n",
    "\n",
    "    # Optionally save the model after each epoch\n",
    "    \n",
    "\n",
    "# Output the completion of training\n",
    "print(\"Training complete and model saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel crashes when trying to do more than 100 ish rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to Early_Predictor\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"Early_Predictor\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Creating an attention mask for the input tokens\n",
    "attention_mask = torch.ones(test_input_tokens.shape, dtype=torch.long)  # Assuming all tokens should be attended to\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    \n",
    "test_output_tokens = model.generate(\n",
    "    test_input_tokens,\n",
    "    max_length=test_input_tokens.shape[1] + 20,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=1,\n",
    "    do_sample=True  # Enable sampling-based generation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance 1\n",
      "Input text: 720 Jump ball: P. Gasol vs. T. Mozgov (D. Rose gains possession):695 N. Miroti misses 2-pt layup from 2 ft:694 Defensive rebound by K. Love:683 K. Love makes 2-pt dunk at rim (assist by L. James):671 D. Rose misses 2-pt jump shot from 3 ft (block by T. Mozgov)\n",
      "Generated text: \n",
      "Actual text: 669 Defensive rebound by T. Mozgov\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance 2\n",
      "Input text: 695 N. Miroti misses 2-pt layup from 2 ft:694 Defensive rebound by K. Love:683 K. Love makes 2-pt dunk at rim (assist by L. James):671 D. Rose misses 2-pt jump shot from 3 ft (block by T. Mozgov):669 Defensive rebound by T. Mozgov\n",
      "Generated text: \n",
      "Actual text: 663 M. Williams makes 2-pt jump shot from 21 ft\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance 3\n",
      "Input text: 694 Defensive rebound by K. Love:683 K. Love makes 2-pt dunk at rim (assist by L. James):671 D. Rose misses 2-pt jump shot from 3 ft (block by T. Mozgov):669 Defensive rebound by T. Mozgov:663 M. Williams makes 2-pt jump shot from 21 ft\n",
      "Generated text: \n",
      "Actual text: 649 P. Gasol misses 2-pt jump shot from 19 ft\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance 4\n",
      "Input text: 683 K. Love makes 2-pt dunk at rim (assist by L. James):671 D. Rose misses 2-pt jump shot from 3 ft (block by T. Mozgov):669 Defensive rebound by T. Mozgov:663 M. Williams makes 2-pt jump shot from 21 ft:649 P. Gasol misses 2-pt jump shot from 19 ft\n",
      "Generated text: \n",
      "Actual text: 647 Defensive rebound by L. James\n",
      "--------------------------------------------------\n",
      "Instance 5\n",
      "Input text: 671 D. Rose misses 2-pt jump shot from 3 ft (block by T. Mozgov):669 Defensive rebound by T. Mozgov:663 M. Williams makes 2-pt jump shot from 21 ft:649 P. Gasol misses 2-pt jump shot from 19 ft:647 Defensive rebound by L. James\n",
      "Generated text: \n",
      "Actual text: 628 L. James makes 2-pt jump shot from 20 ft\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "# Choose a number of instances to test, for example, 5\n",
    "num_test_instances = min(len(df), 5)\n",
    "\n",
    "for idx in range(num_test_instances):\n",
    "    test_row = df.iloc[idx]\n",
    "    test_input_text = test_row['GameText']\n",
    "    actual_label_text = test_row['Label']  # The actual label (text) you want to predict\n",
    "\n",
    "    # Tokenize test input\n",
    "    test_input_tokens = tokenizer.encode(test_input_text, return_tensors='pt').to(device)\n",
    "\n",
    "    # Assuming your model and tokenizer are correctly set up to handle single instances\n",
    "    attention_mask = torch.ones(test_input_tokens.shape, dtype=torch.long, device=test_input_tokens.device)\n",
    "\n",
    "    # Generate text using the model\n",
    "    with torch.no_grad():\n",
    "        test_output_tokens = model.generate(\n",
    "            input_ids=test_input_tokens,\n",
    "            attention_mask=attention_mask,  # Now explicitly provided\n",
    "            max_length=test_input_tokens.shape[1] + 20,  # Adjust as necessary\n",
    "            temperature=1.0,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=True  # Enable sampling for more varied output\n",
    "        )\n",
    "\n",
    "        # Decode generated tokens back to text\n",
    "        generated_text = tokenizer.decode(test_output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "        # Display the input, actual, and generated texts\n",
    "        print(f\"Instance {idx+1}\")\n",
    "        print(f\"Input text: {test_input_text}\")\n",
    "        print(f\"Generated text: {generated_text[len(tokenizer.decode(test_input_tokens[0], skip_special_tokens=True)):]}\")  # Show generated continuation\n",
    "        print(f\"Actual text: {actual_label_text}\")\n",
    "        print(\"-\" * 50)  # Separator for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: './Simple_Quarter'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/NLP/ggalusha/project/project-movies/nlp-virtual/lib/python3.10/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/NLP/ggalusha/project/project-movies/nlp-virtual/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:111\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 111\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/NLP/ggalusha/project/project-movies/nlp-virtual/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:165\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m     )\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './Simple_Quarter'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GPT2Tokenizer, GPT2LMHeadModel\n\u001b[0;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./Simple_Quarter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Simple_Quarter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/NLP/ggalusha/project/project-movies/nlp-virtual/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2007\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vocab_files:\n\u001b[1;32m   2005\u001b[0m     \u001b[38;5;66;03m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[39;00m\n\u001b[1;32m   2006\u001b[0m     fast_tokenizer_file \u001b[38;5;241m=\u001b[39m FULL_TOKENIZER_FILE\n\u001b[0;32m-> 2007\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2017\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2018\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2019\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2020\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2021\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2022\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2023\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2024\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   2025\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/NLP/ggalusha/project/project-movies/nlp-virtual/lib/python3.10/site-packages/transformers/utils/hub.py:462\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere was a specific connection error when trying to load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HFValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 462\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    463\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncorrect path_or_model_id: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    464\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n",
      "\u001b[0;31mOSError\u001b[0m: Incorrect path_or_model_id: './Simple_Quarter'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./Simple_Quarter\")\n",
    "\n",
    "# Load the model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./Simple_Quarter\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
