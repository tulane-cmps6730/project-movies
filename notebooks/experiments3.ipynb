{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 names  release_year maturity_rating duration  \\\n",
      "0        Mission Majnu          2023        U/A 16+     2h 9m   \n",
      "1               Cirkus          2022         U/A 7+    2h 14m   \n",
      "2  Gangubai Kathiawadi          2022        U/A 16+    2h 33m   \n",
      "3              Thunivu          2023        U/A 16+    2h 22m   \n",
      "4    Bhool Bhulaiyaa 2          2022        U/A 13+    2h 21m   \n",
      "\n",
      "                                         description  \\\n",
      "0  In the 1970s, an undercover Indian spy takes o...   \n",
      "1  Chaos and comedy take the spotlight when a rin...   \n",
      "2  Duped and sold to a brothel, a young woman fea...   \n",
      "3  A major bank heist takes an unnerving turn whe...   \n",
      "4  When strangers Reet and Ruhan cross paths, the...   \n",
      "\n",
      "                                               genre         mood  \\\n",
      "0  ['Spy Movies', 'Hindi-Language Movies', 'Bolly...  Suspenseful   \n",
      "1  ['Hindi-Language Movies', 'Bollywood Movies', ...        Goofy   \n",
      "2  ['Hindi-Language Movies', 'Movies Based on Boo...  Provocative   \n",
      "3             ['Crime Movies', 'Action & Adventure']     Exciting   \n",
      "4  ['Hindi-Language Movies', 'Bollywood Movies', ...     Offbeat,   \n",
      "\n",
      "                                                cast  \\\n",
      "0  ['Sidharth Malhotra', 'Rashmika Mandanna', 'Pa...   \n",
      "1  ['Ranveer Singh', 'Varun Sharma', 'Pooja Hegde...   \n",
      "2  ['Alia Bhatt', 'Vijay Raaz', 'Seema Pahwa', 'A...   \n",
      "3  ['Ajith Kumar', 'Manju Warrier', 'Samuthirakan...   \n",
      "4  ['Tabu', 'Kartik Aaryan', 'Kiara Advani', 'Raj...   \n",
      "\n",
      "                           subtitles  \\\n",
      "0  ['English,', 'English,', 'Hindi']   \n",
      "1              ['English,', 'Hindi']   \n",
      "2              ['English,', 'Hindi']   \n",
      "3              ['English,', 'Hindi']   \n",
      "4              ['English,', 'Hindi']   \n",
      "\n",
      "                                               audio  \n",
      "0  ['English,', 'Hindi - Audio Description,', 'Hi...  \n",
      "1                               ['Hindi [Original]']  \n",
      "2                    ['Hindi [Original],', 'Telugu']  \n",
      "3                               ['Tamil [Original]']  \n",
      "4                               ['Hindi [Original]']  \n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 560 entries, 0 to 559\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   names            560 non-null    object\n",
      " 1   release_year     560 non-null    int64 \n",
      " 2   maturity_rating  560 non-null    object\n",
      " 3   duration         560 non-null    object\n",
      " 4   description      560 non-null    object\n",
      " 5   genre            560 non-null    object\n",
      " 6   mood             517 non-null    object\n",
      " 7   cast             560 non-null    object\n",
      " 8   subtitles        560 non-null    object\n",
      " 9   audio            560 non-null    object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 43.9+ KB\n",
      "\n",
      "Descriptive Statistics:\n",
      "       release_year\n",
      "count    560.000000\n",
      "mean    2016.521429\n",
      "std        6.282517\n",
      "min     1993.000000\n",
      "25%     2012.000000\n",
      "50%     2019.000000\n",
      "75%     2022.000000\n",
      "max     2023.000000\n",
      "\n",
      "Missing Values:\n",
      "names               0\n",
      "release_year        0\n",
      "maturity_rating     0\n",
      "duration            0\n",
      "description         0\n",
      "genre               0\n",
      "mood               43\n",
      "cast                0\n",
      "subtitles           0\n",
      "audio               0\n",
      "dtype: int64\n",
      "\n",
      "Unique Values per Column:\n",
      "names: 245\n",
      "release_year: 27\n",
      "maturity_rating: 5\n",
      "duration: 93\n",
      "description: 245\n",
      "genre: 127\n",
      "mood: 46\n",
      "cast: 240\n",
      "subtitles: 25\n",
      "audio: 43\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('Data/netfix_cleaned.csv')\n",
    "\n",
    "# Display the top 5 rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Basic information about the dataset\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()\n",
    "\n",
    "# Descriptive statistics for numeric columns\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Checking for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check the distribution of a categorical variable (if applicable)\n",
    "if 'category_column_name' in df.columns:\n",
    "    print(\"\\nCategory Distribution:\")\n",
    "    print(df['category_column_name'].value_counts())\n",
    "\n",
    "# Feel free to replace 'category_column_name' with an actual column name from your dataset\n",
    "# that you're interested to explore.\n",
    "\n",
    "# Another useful exploration is to see the number of unique values in each column\n",
    "print(\"\\nUnique Values per Column:\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].nunique()}\")\n",
    "\n",
    "# Displaying the distribution of numeric data\n",
    "# Importing necessary libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the visualisation style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plotting the distribution of a numeric variable (if applicable)\n",
    "if 'numeric_column_name' in df.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['numeric_column_name'], kde=True, bins=30)\n",
    "    plt.title('Distribution of Numeric Column')\n",
    "    plt.xlabel('Numeric Column Name')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "# Remember to replace 'numeric_column_name' with an actual numeric column name from your dataset.\n",
    "df[\"mood\"] = df[\"mood\"].fillna(\"Unlabeled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duration.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hours and minutes from the duration column\n",
    "df['hours'] = df['duration'].str.extract('(\\d+)h').fillna(0)\n",
    "df['minutes'] = df['duration'].str.extract('(\\d+)m').fillna(0)\n",
    "\n",
    "# Convert the hours and minutes to integers\n",
    "df['hours'] = df['hours'].astype(int)\n",
    "df['minutes'] = df['minutes'].astype(int)\n",
    "\n",
    "# Calculate the total minutes\n",
    "df['total_minutes'] = df['hours'] * 60 + df['minutes']\n",
    "\n",
    "# Now you can drop the 'hours' and 'minutes' columns if they are not needed\n",
    "df = df.drop(['hours', 'minutes'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W2v works less well thatn IF-IVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "# Load GloVe model\n",
    "w2v = api.load('glove-wiki-gigaword-50')\n",
    "\n",
    "# Adjust the function to work with the loaded model\n",
    "def sentence_to_vec(sentence, model, num_features):\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    index2word_set = set(model.key_to_index)  # Updated for gensim 4.0.0 and later\n",
    "    \n",
    "    for word in sentence.split():\n",
    "        if word in index2word_set:\n",
    "            nwords += 1\n",
    "            featureVec = np.add(featureVec, model[word])  # Corrected access to word vector\n",
    "    \n",
    "    if nwords > 0:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "# Since you're using glove-wiki-gigaword-50, each word vector has 50 dimensions\n",
    "num_features = 50\n",
    "\n",
    "# Apply the function to each description\n",
    "# Assuming 'df' is your DataFrame and 'description' is the column with text data\n",
    "vec_descriptions = np.array([sentence_to_vec(sentence, w2v, num_features) for sentence in df['description']])\n",
    "\n",
    "# `vec_descriptions` is a 2D numpy array where each row represents a sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.610378  ,  0.21967086,  0.0120224 , ...,  0.30045193,\n",
       "        -0.19191389, -0.09676496],\n",
       "       [ 0.13837972,  0.14362648, -0.41620106, ..., -0.43784407,\n",
       "        -0.20476098, -0.2689774 ],\n",
       "       [ 0.21785122,  0.17795451, -0.11302451, ..., -0.50542885,\n",
       "        -0.00165171, -0.18363637],\n",
       "       ...,\n",
       "       [ 0.29497018,  0.3061172 , -0.3795322 , ..., -0.26171908,\n",
       "        -0.093453  , -0.107467  ],\n",
       "       [ 0.28957808, -0.05298619,  0.04495952, ..., -0.1645367 ,\n",
       "         0.15370815, -0.15710595],\n",
       "       [ 0.21053235,  0.3161477 , -0.30063367, ..., -0.4940572 ,\n",
       "         0.0823618 , -0.12313429]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gavingalusha/anaconda3/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/Users/gavingalusha/anaconda3/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/Users/gavingalusha/anaconda3/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/var/folders/h6/ds76bqyd2t3gnfgtmgzjkxy80000gn/T/ipykernel_97248/4063524987.py:32: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:248.)\n",
      "  mood_tensor = torch.tensor([encoded_mood])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "encoded_release_year = one_hot_encoder.fit_transform(df[['release_year']])\n",
    "encoded_maturity_rating = one_hot_encoder.fit_transform(df[['maturity_rating']])\n",
    "\n",
    "# Normalize 'duration' (convert to total minutes)\n",
    "\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "encoded_description = vec.fit_transform(df.description).toarray() \n",
    "\n",
    "\n",
    "# Multi-hot encoding for 'genre' and 'cast'\n",
    "mlb_genre = MultiLabelBinarizer()\n",
    "mlb_cast = MultiLabelBinarizer()\n",
    "encoded_genre = mlb_genre.fit_transform(df['genre'])\n",
    "encoded_cast = mlb_cast.fit_transform(df['cast'])\n",
    "\n",
    "# One-hot encoding for 'mood'\n",
    "encoded_mood = one_hot_encoder.fit_transform(df[['mood']])\n",
    "\n",
    "# Combine all features into a features tensor\n",
    "features = np.hstack([encoded_release_year, encoded_maturity_rating, df[['total_minutes']].values, encoded_genre, encoded_cast, encoded_description])\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "\n",
    "features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "mood_tensor = torch.tensor([encoded_mood])\n",
    "print(encoded_description[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2348"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tensor.shape\n",
    "input_shape = features_tensor.shape[1]\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([560, 2348])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([560, 47])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mood_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "# Assuming features_tensor contains your features and mood_tensor is your one-hot encoded mood column\n",
    "mood_classes = mood_tensor.shape[1]  # Number of unique mood classes\n",
    "features_tensor = features_tensor \n",
    " # Your existing feature tensor\n",
    "mood_tensor = mood_tensor.squeeze(0)  # Your existing target tensor for mood\n",
    "\n",
    "# Create Dataset\n",
    "dataset = TensorDataset(features_tensor, mood_tensor)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([32, 47])) must be the same as input size (torch.Size([32, 560]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     32\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m---> 33\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/loss.py:720\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m                                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m                                              \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/functional.py:3163\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3160\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m-> 3163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m   3165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum)\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([32, 47])) must be the same as input size (torch.Size([32, 560]))"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MoodPredictor(nn.Module):\n",
    "    def __init__(self, input_size, mood_classes):\n",
    "        super(MoodPredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, mood_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Adjust `input_size` and `mood_classes` according to your dataset\n",
    "model = MoodPredictor(input_shape, mood_classes)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Validation step\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 14.29%\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels = torch.max(labels.data, 1)  # assuming your labels are also one-hot encoded\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a logistic regression model\n",
    "# Using a pipeline to include standard scaling of the data\n",
    "# Solver 'lbfgs' is a good default choice; you might need to increase `max_iter` for convergence\n",
    "log = make_pipeline(StandardScaler(), LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000))\n",
    "\n",
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_np = features_tensor.numpy()\n",
    "\n",
    "# Assuming 'mood_tensor' needs to be a 1D numpy array for scikit-learn\n",
    "# First, ensure 'mood_tensor' is correctly shaped. It looks like you might have an extra dimension.\n",
    "# If 'encoded_mood' is your target variable in the correct format, you might not need to wrap it in an additional tensor.\n",
    "# Here's how to reshape and convert it assuming 'encoded_mood' is a 2D numpy array where each row is a one-hot encoded target.\n",
    "mood_np = np.argmax(encoded_mood, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8035714285714286\n",
      "Confusion Matrix:\n",
      "[[2 0 0 ... 0 0 0]\n",
      " [0 2 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 2 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 2]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       0.67      0.33      0.44         6\n",
      "           3       1.00      1.00      1.00         1\n",
      "           7       1.00      1.00      1.00         1\n",
      "           8       1.00      1.00      1.00         7\n",
      "           9       0.51      0.95      0.67        20\n",
      "          11       1.00      0.80      0.89         5\n",
      "          12       1.00      0.50      0.67         2\n",
      "          13       0.83      0.83      0.83         6\n",
      "          14       1.00      1.00      1.00         3\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       1.00      1.00      1.00         3\n",
      "          18       1.00      1.00      1.00         5\n",
      "          19       1.00      1.00      1.00         1\n",
      "          20       1.00      1.00      1.00         1\n",
      "          21       1.00      1.00      1.00         1\n",
      "          23       1.00      1.00      1.00         4\n",
      "          24       1.00      1.00      1.00         1\n",
      "          25       1.00      0.50      0.67         2\n",
      "          27       1.00      1.00      1.00         3\n",
      "          28       1.00      1.00      1.00         1\n",
      "          29       1.00      1.00      1.00         4\n",
      "          30       1.00      1.00      1.00         3\n",
      "          31       0.00      0.00      0.00         1\n",
      "          32       0.00      0.00      0.00         1\n",
      "          33       0.00      0.00      0.00         1\n",
      "          35       1.00      1.00      1.00         1\n",
      "          36       1.00      1.00      1.00         3\n",
      "          38       0.00      0.00      0.00         1\n",
      "          39       1.00      0.80      0.89         5\n",
      "          42       0.80      0.50      0.62         8\n",
      "          43       0.00      0.00      0.00         1\n",
      "          44       1.00      0.50      0.67         4\n",
      "          45       1.00      1.00      1.00         1\n",
      "          46       1.00      0.67      0.80         3\n",
      "\n",
      "    accuracy                           0.80       112\n",
      "   macro avg       0.79      0.73      0.75       112\n",
      "weighted avg       0.83      0.80      0.79       112\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gavingalusha/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gavingalusha/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gavingalusha/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gavingalusha/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gavingalusha/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/gavingalusha/anaconda3/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_np, mood_np, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "  # Increase max_iter if convergence issues occur\n",
    "\n",
    "# Fit the model on the training data\n",
    "log.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = log.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "conf_matrix = confusion_matrix(y_test, predictions)\n",
    "class_report = classification_report(y_test, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far logistic regression has performed better than the Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
