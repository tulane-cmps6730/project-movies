{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 names  release_year maturity_rating duration  \\\n",
      "0        Mission Majnu          2023        U/A 16+     2h 9m   \n",
      "1               Cirkus          2022         U/A 7+    2h 14m   \n",
      "2  Gangubai Kathiawadi          2022        U/A 16+    2h 33m   \n",
      "3              Thunivu          2023        U/A 16+    2h 22m   \n",
      "4    Bhool Bhulaiyaa 2          2022        U/A 13+    2h 21m   \n",
      "\n",
      "                                         description  \\\n",
      "0  In the 1970s, an undercover Indian spy takes o...   \n",
      "1  Chaos and comedy take the spotlight when a rin...   \n",
      "2  Duped and sold to a brothel, a young woman fea...   \n",
      "3  A major bank heist takes an unnerving turn whe...   \n",
      "4  When strangers Reet and Ruhan cross paths, the...   \n",
      "\n",
      "                                               genre         mood  \\\n",
      "0  ['Spy Movies', 'Hindi-Language Movies', 'Bolly...  Suspenseful   \n",
      "1  ['Hindi-Language Movies', 'Bollywood Movies', ...        Goofy   \n",
      "2  ['Hindi-Language Movies', 'Movies Based on Boo...  Provocative   \n",
      "3             ['Crime Movies', 'Action & Adventure']     Exciting   \n",
      "4  ['Hindi-Language Movies', 'Bollywood Movies', ...     Offbeat,   \n",
      "\n",
      "                                                cast  \\\n",
      "0  ['Sidharth Malhotra', 'Rashmika Mandanna', 'Pa...   \n",
      "1  ['Ranveer Singh', 'Varun Sharma', 'Pooja Hegde...   \n",
      "2  ['Alia Bhatt', 'Vijay Raaz', 'Seema Pahwa', 'A...   \n",
      "3  ['Ajith Kumar', 'Manju Warrier', 'Samuthirakan...   \n",
      "4  ['Tabu', 'Kartik Aaryan', 'Kiara Advani', 'Raj...   \n",
      "\n",
      "                           subtitles  \\\n",
      "0  ['English,', 'English,', 'Hindi']   \n",
      "1              ['English,', 'Hindi']   \n",
      "2              ['English,', 'Hindi']   \n",
      "3              ['English,', 'Hindi']   \n",
      "4              ['English,', 'Hindi']   \n",
      "\n",
      "                                               audio  \n",
      "0  ['English,', 'Hindi - Audio Description,', 'Hi...  \n",
      "1                               ['Hindi [Original]']  \n",
      "2                    ['Hindi [Original],', 'Telugu']  \n",
      "3                               ['Tamil [Original]']  \n",
      "4                               ['Hindi [Original]']  \n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 560 entries, 0 to 559\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   names            560 non-null    object\n",
      " 1   release_year     560 non-null    int64 \n",
      " 2   maturity_rating  560 non-null    object\n",
      " 3   duration         560 non-null    object\n",
      " 4   description      560 non-null    object\n",
      " 5   genre            560 non-null    object\n",
      " 6   mood             517 non-null    object\n",
      " 7   cast             560 non-null    object\n",
      " 8   subtitles        560 non-null    object\n",
      " 9   audio            560 non-null    object\n",
      "dtypes: int64(1), object(9)\n",
      "memory usage: 43.9+ KB\n",
      "\n",
      "Descriptive Statistics:\n",
      "       release_year\n",
      "count    560.000000\n",
      "mean    2016.521429\n",
      "std        6.282517\n",
      "min     1993.000000\n",
      "25%     2012.000000\n",
      "50%     2019.000000\n",
      "75%     2022.000000\n",
      "max     2023.000000\n",
      "\n",
      "Missing Values:\n",
      "names               0\n",
      "release_year        0\n",
      "maturity_rating     0\n",
      "duration            0\n",
      "description         0\n",
      "genre               0\n",
      "mood               43\n",
      "cast                0\n",
      "subtitles           0\n",
      "audio               0\n",
      "dtype: int64\n",
      "\n",
      "Unique Values per Column:\n",
      "names: 245\n",
      "release_year: 27\n",
      "maturity_rating: 5\n",
      "duration: 93\n",
      "description: 245\n",
      "genre: 127\n",
      "mood: 46\n",
      "cast: 240\n",
      "subtitles: 25\n",
      "audio: 43\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('Data/netfix_cleaned.csv')\n",
    "\n",
    "# Display the top 5 rows of the dataframe\n",
    "print(df.head())\n",
    "\n",
    "# Basic information about the dataset\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()\n",
    "\n",
    "# Descriptive statistics for numeric columns\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Checking for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check the distribution of a categorical variable (if applicable)\n",
    "if 'category_column_name' in df.columns:\n",
    "    print(\"\\nCategory Distribution:\")\n",
    "    print(df['category_column_name'].value_counts())\n",
    "\n",
    "# Feel free to replace 'category_column_name' with an actual column name from your dataset\n",
    "# that you're interested to explore.\n",
    "\n",
    "# Another useful exploration is to see the number of unique values in each column\n",
    "print(\"\\nUnique Values per Column:\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].nunique()}\")\n",
    "\n",
    "# Displaying the distribution of numeric data\n",
    "# Importing necessary libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the visualisation style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plotting the distribution of a numeric variable (if applicable)\n",
    "if 'numeric_column_name' in df.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['numeric_column_name'], kde=True, bins=30)\n",
    "    plt.title('Distribution of Numeric Column')\n",
    "    plt.xlabel('Numeric Column Name')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "# Remember to replace 'numeric_column_name' with an actual numeric column name from your dataset.\n",
    "df[\"mood\"] = df[\"mood\"].fillna(\"Unlabeled\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duration.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hours and minutes from the duration column\n",
    "df['hours'] = df['duration'].str.extract('(\\d+)h').fillna(0)\n",
    "df['minutes'] = df['duration'].str.extract('(\\d+)m').fillna(0)\n",
    "\n",
    "# Convert the hours and minutes to integers\n",
    "df['hours'] = df['hours'].astype(int)\n",
    "df['minutes'] = df['minutes'].astype(int)\n",
    "\n",
    "# Calculate the total minutes\n",
    "df['total_minutes'] = df['hours'] * 60 + df['minutes']\n",
    "\n",
    "# Now you can drop the 'hours' and 'minutes' columns if they are not needed\n",
    "df = df.drop(['hours', 'minutes'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W2v works less well thatn IF-IVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "# Load GloVe model\n",
    "w2v = api.load('glove-wiki-gigaword-50')\n",
    "\n",
    "# Adjust the function to work with the loaded model\n",
    "def sentence_to_vec(sentence, model, num_features):\n",
    "    featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    index2word_set = set(model.key_to_index)  # Updated for gensim 4.0.0 and later\n",
    "    \n",
    "    for word in sentence.split():\n",
    "        if word in index2word_set:\n",
    "            nwords += 1\n",
    "            featureVec = np.add(featureVec, model[word])  # Corrected access to word vector\n",
    "    \n",
    "    if nwords > 0:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "# Since you're using glove-wiki-gigaword-50, each word vector has 50 dimensions\n",
    "num_features = 50\n",
    "\n",
    "# Apply the function to each description\n",
    "# Assuming 'df' is your DataFrame and 'description' is the column with text data\n",
    "vec_descriptions = np.array([sentence_to_vec(sentence, w2v, num_features) for sentence in df['description']])\n",
    "\n",
    "# `vec_descriptions` is a 2D numpy array where each row represents a sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.610378  ,  0.21967086,  0.0120224 , ...,  0.30045193,\n",
       "        -0.19191389, -0.09676496],\n",
       "       [ 0.13837972,  0.14362648, -0.41620106, ..., -0.43784407,\n",
       "        -0.20476098, -0.2689774 ],\n",
       "       [ 0.21785122,  0.17795451, -0.11302451, ..., -0.50542885,\n",
       "        -0.00165171, -0.18363637],\n",
       "       ...,\n",
       "       [ 0.29497018,  0.3061172 , -0.3795322 , ..., -0.26171908,\n",
       "        -0.093453  , -0.107467  ],\n",
       "       [ 0.28957808, -0.05298619,  0.04495952, ..., -0.1645367 ,\n",
       "         0.15370815, -0.15710595],\n",
       "       [ 0.21053235,  0.3161477 , -0.30063367, ..., -0.4940572 ,\n",
       "         0.0823618 , -0.12313429]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gavingalusha/anaconda3/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/Users/gavingalusha/anaconda3/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n",
      "/Users/gavingalusha/anaconda3/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:972: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder, MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "encoded_release_year = one_hot_encoder.fit_transform(df[['release_year']])\n",
    "encoded_maturity_rating = one_hot_encoder.fit_transform(df[['maturity_rating']])\n",
    "\n",
    "# Normalize 'duration' (convert to total minutes)\n",
    "\n",
    "\n",
    "vec = TfidfVectorizer()\n",
    "encoded_description = vec.fit_transform(df.description).toarray() \n",
    "\n",
    "\n",
    "# Multi-hot encoding for 'genre' and 'cast'\n",
    "mlb_genre = MultiLabelBinarizer()\n",
    "mlb_cast = MultiLabelBinarizer()\n",
    "encoded_genre = mlb_genre.fit_transform(df['genre'])\n",
    "encoded_cast = mlb_cast.fit_transform(df['cast'])\n",
    "\n",
    "# One-hot encoding for 'mood'\n",
    "encoded_mood = one_hot_encoder.fit_transform(df[['mood']])\n",
    "\n",
    "# Combine all features into a features tensor\n",
    "features = np.hstack([encoded_release_year, encoded_maturity_rating, df[['total_minutes']].values, encoded_genre, encoded_cast, encoded_description])\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "\n",
    "features_tensor = torch.tensor(features, dtype=torch.float32)\n",
    "mood_tensor = torch.tensor([encoded_mood])\n",
    "print(encoded_description[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([560, 2348])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Size mismatch between tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m mood_tensor \u001b[38;5;241m=\u001b[39m mood_tensor  \u001b[38;5;66;03m# Your existing target tensor for mood\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Create Dataset\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmood_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Split the dataset into training and validation sets\u001b[39;00m\n\u001b[1;32m     15\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/utils/data/dataset.py:192\u001b[0m, in \u001b[0;36mTensorDataset.__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mtensors: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(tensors[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize mismatch between tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors\n",
      "\u001b[0;31mAssertionError\u001b[0m: Size mismatch between tensors"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "# Assuming features_tensor contains your features and mood_tensor is your one-hot encoded mood column\n",
    "#mood_classes = mood_tensor.shape[1]  # Number of unique mood classes\n",
    "features_tensor = features_tensor  # Your existing feature tensor\n",
    "mood_tensor = mood_tensor  # Your existing target tensor for mood\n",
    "\n",
    "# Create Dataset\n",
    "dataset = TensorDataset(features_tensor, mood_tensor)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.1673\n",
      "Epoch 2/10, Loss: 0.1063\n",
      "Epoch 3/10, Loss: 0.0993\n",
      "Epoch 4/10, Loss: 0.0972\n",
      "Epoch 5/10, Loss: 0.0972\n",
      "Epoch 6/10, Loss: 0.0962\n",
      "Epoch 7/10, Loss: 0.0959\n",
      "Epoch 8/10, Loss: 0.0955\n",
      "Epoch 9/10, Loss: 0.0953\n",
      "Epoch 10/10, Loss: 0.0948\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Neural network architecture\n",
    "class MoodPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MoodPredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(204, 128)  # Adjust the input size to match your feature tensor size\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, mood_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = MoodPredictor()\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.BCEWithLogitsLoss()  # Use BCEWithLogitsLoss for multi-label classification\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation step\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {val_loss:.4f}')\n",
    "\n",
    "# Check the accuracy on validation set after training if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 14.29%\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels = torch.max(labels.data, 1)  # assuming your labels are also one-hot encoded\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a logistic regression model\n",
    "# Using a pipeline to include standard scaling of the data\n",
    "# Solver 'lbfgs' is a good default choice; you might need to increase `max_iter` for convergence\n",
    "model = make_pipeline(StandardScaler(), LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000))\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x7f799d7f8910>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset\n",
    "val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
