{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "nba_url = 'https://www.reddit.com/r/nba.json'\n",
    "lebron_url = 'https://www.reddit.com/r/lebron.json'\n",
    "micheal_url = 'https://www.reddit.com/r/michaeljordan.json'\n",
    "kobe_url = \"https://www.reddit.com/r/KobeBryant24.json\"\n",
    "header = {'User-agent': 'subreddit get requests'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to get num pages of posts from a subreddit, start collecting at a defined after\n",
    "def reddit_scraper(url, num, after = None):\n",
    "    posts = []\n",
    "    # loop through the num pages, each subreddit .json returns 25 posts \n",
    "    for page in range(num):\n",
    "        # initiate params modifier for posts if there no defined after\n",
    "        if after == None:\n",
    "            params = {}\n",
    "        # add in after id for each loop following to ensure no duplicate posts\n",
    "        else:\n",
    "            params = {'after': after}\n",
    "        # call our get request for the posts\n",
    "        res = requests.get(url, params=params, headers=header)\n",
    "        # check status code, 200 means posts were successfully downloaded\n",
    "        if res.status_code == 200:\n",
    "            # convert request to .json\n",
    "            new_json = res.json()\n",
    "            # extend list from the 'children' dictionary for each request\n",
    "            posts.extend(new_json['data']['children'])\n",
    "            # update after id\n",
    "            after = new_json['data']['after']\n",
    "        else:\n",
    "            # print status code if not 200\n",
    "            print(res.status_code)\n",
    "            break\n",
    "        # wait 1 second\n",
    "        time.sleep(1)\n",
    "        \n",
    "    # create a new dataframe with the 'data' from each post\n",
    "    new_df = pd.DataFrame([post['data'] for post in posts])\n",
    "    \n",
    "    # print final value of after\n",
    "    print(f'Final value of after parameter: {after}')\n",
    "    \n",
    "    # return the dataframe\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final value of after parameter: t3_zz7k1n\n",
      "Final value of after parameter: t3_4uvx6f\n",
      "Final value of after parameter: t3_13m4sin\n"
     ]
    }
   ],
   "source": [
    "lebron_df = reddit_scraper(lebron_url, 10)\n",
    "jordan_df = reddit_scraper(micheal_url, 10)\n",
    "kobe_df = reddit_scraper(kobe_url, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    df = df[['selftext', 'title', 'subreddit']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [extract_features(lebron_df), extract_features(jordan_df), extract_features(kobe_df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df = big_df.dropna(subset=['selftext', 'title'], how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize two separate vectorizers\n",
    "tfidf_title = TfidfVectorizer(max_df=0.95, min_df=2, max_features=5000)\n",
    "tfidf_selftext = TfidfVectorizer(max_df=0.95, min_df=2, max_features=5000)\n",
    "tfidf = TfidfVectorizer(max_df=0.95, min_df=2, max_features=5000)\n",
    "\n",
    "# Fit and transform separately\n",
    "title_vecs = tfidf_title.fit_transform(big_df['title'])\n",
    "selftext_vecs = tfidf_selftext.fit_transform(big_df['selftext'])\n",
    "\n",
    "# Combine the vectors\n",
    "X = hstack([title_vecs, selftext_vecs])\n",
    "y = pd.get_dummies(big_df[\"subreddit\"]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(big_df, test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "num_labels = len(train_df['subreddit'].unique())\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['selftext', 'title', 'subreddit', '__index_level_0__']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1f3f84438245b9b81066324df0c75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/676 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6dcf4b7acac4c62ba6166be891b7687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/76 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"selftext\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    evaluate_during_training=True,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb72272fa6fb46758dc1e79972506f0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90cb62cdae945cda8229d942ce0863b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51349a8978542ae8e14ad35420386a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60b4eebf1e64938a2a02d06cc7fc429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3218c13c234c74be5b6c2bbf20abb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Kobe_Jordan_Lebron.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>combined_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n</td>\n",
       "      <td>LEBRON ON SACRIFICES YOU HAVE TO MAKE TO BE TH...</td>\n",
       "      <td>lebron</td>\n",
       "      <td>LEBRON ON SACRIFICES YOU HAVE TO MAKE TO BE TH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>LeBron poster I designed! Instagram is @csc.dzn💫</td>\n",
       "      <td>lebron</td>\n",
       "      <td>LeBron poster I designed! Instagram is @csc.dzn💫</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Where can I buy this?</td>\n",
       "      <td>lebron</td>\n",
       "      <td>Where can I buy this?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As you all  know, king LBJ dominated the game ...</td>\n",
       "      <td>lebron legacy question</td>\n",
       "      <td>lebron</td>\n",
       "      <td>lebron legacy question As you all  know, king ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>what if lebron went to the bulls?</td>\n",
       "      <td>lebron</td>\n",
       "      <td>what if lebron went to the bulls?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>NaN</td>\n",
       "      <td>This is pretty wild!</td>\n",
       "      <td>KobeBryant24</td>\n",
       "      <td>This is pretty wild!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>So I have a picture of me when I was a year an...</td>\n",
       "      <td>Looking for some help!!</td>\n",
       "      <td>KobeBryant24</td>\n",
       "      <td>Looking for some help!! So I have a picture of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Kobe’s 8 Passengers</td>\n",
       "      <td>KobeBryant24</td>\n",
       "      <td>Kobe’s 8 Passengers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>&amp;amp;#x200B;\\n\\nhttps://preview.redd.it/n15rpi...</td>\n",
       "      <td>Made this [ig-@wassuppixel]</td>\n",
       "      <td>KobeBryant24</td>\n",
       "      <td>Made this [ig-@wassuppixel] &amp;amp;#x200B;\\n\\nht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Kobe Bryant 🏀🐍✒️</td>\n",
       "      <td>KobeBryant24</td>\n",
       "      <td>Kobe Bryant 🏀🐍✒️</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>752 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              selftext  \\\n",
       "0                                                   \\n   \n",
       "1                                                  NaN   \n",
       "2                                                  NaN   \n",
       "3    As you all  know, king LBJ dominated the game ...   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "747                                                NaN   \n",
       "748  So I have a picture of me when I was a year an...   \n",
       "749                                                NaN   \n",
       "750  &amp;#x200B;\\n\\nhttps://preview.redd.it/n15rpi...   \n",
       "751                                                NaN   \n",
       "\n",
       "                                                 title     subreddit  \\\n",
       "0    LEBRON ON SACRIFICES YOU HAVE TO MAKE TO BE TH...        lebron   \n",
       "1     LeBron poster I designed! Instagram is @csc.dzn💫        lebron   \n",
       "2                                Where can I buy this?        lebron   \n",
       "3                               lebron legacy question        lebron   \n",
       "4                    what if lebron went to the bulls?        lebron   \n",
       "..                                                 ...           ...   \n",
       "747                               This is pretty wild!  KobeBryant24   \n",
       "748                            Looking for some help!!  KobeBryant24   \n",
       "749                                Kobe’s 8 Passengers  KobeBryant24   \n",
       "750                        Made this [ig-@wassuppixel]  KobeBryant24   \n",
       "751                                   Kobe Bryant 🏀🐍✒️  KobeBryant24   \n",
       "\n",
       "                                         combined_text  \n",
       "0    LEBRON ON SACRIFICES YOU HAVE TO MAKE TO BE TH...  \n",
       "1    LeBron poster I designed! Instagram is @csc.dzn💫   \n",
       "2                               Where can I buy this?   \n",
       "3    lebron legacy question As you all  know, king ...  \n",
       "4                   what if lebron went to the bulls?   \n",
       "..                                                 ...  \n",
       "747                              This is pretty wild!   \n",
       "748  Looking for some help!! So I have a picture of...  \n",
       "749                               Kobe’s 8 Passengers   \n",
       "750  Made this [ig-@wassuppixel] &amp;#x200B;\\n\\nht...  \n",
       "751                                  Kobe Bryant 🏀🐍✒️   \n",
       "\n",
       "[752 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Example DataFrame\n",
    "\n",
    "# Placeholder for the output features\n",
    "features = []\n",
    "\n",
    "# Disable gradient calculation for inference\n",
    "with torch.no_grad():\n",
    "    for text in df['combined_text']:\n",
    "        # Tokenize text and convert to tensors\n",
    "        encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        \n",
    "        # Get model output\n",
    "        output = model(**encoded_input)\n",
    "        \n",
    "        # The `last_hidden_state` is the sequence of hidden-states at the output of the last layer\n",
    "        # You might want to use `output.pooler_output` for getting the [CLS] token representation which is commonly used for classification tasks\n",
    "        # Here, we're using the mean of the last hidden state as the feature representation\n",
    "        feature = output.last_hidden_state.mean(dim=1)\n",
    "        \n",
    "        # Append the feature representation to our list\n",
    "        features.append(feature)\n",
    "\n",
    "# Convert the list of tensors to a single tensor\n",
    "features_tensor = torch.cat(features, dim=0)\n",
    "\n",
    "# At this point, `features_tensor` contains the feature representation for each text in the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0565,  0.0135, -0.0037,  ...,  0.5759,  0.3810,  0.2654],\n",
       "        [ 0.2761,  0.0664, -0.0598,  ...,  0.1589,  0.3966,  0.2621],\n",
       "        [ 0.1405,  0.0747,  0.0179,  ...,  0.0734,  0.2343,  0.0433],\n",
       "        ...,\n",
       "        [ 0.3011, -0.2029, -0.2811,  ...,  0.1102,  0.1982, -0.3990],\n",
       "        [ 0.1808, -0.0697,  0.1216,  ...,  0.1223,  0.1646,  0.2742],\n",
       "        [ 0.2529, -0.2798,  0.1280,  ...,  0.2531,  0.3956,  0.6909]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load the dataset (assuming it's already loaded in df)\n",
    "# df = pd.read_csv(\"your_dataset.csv\")\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenization and encoding the dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Assuming 'subreddit' is categorical and needs to be converted to integers\n",
    "label_dict = {label: index for index, label in enumerate(df['subreddit'].unique())}\n",
    "df['label'] = df['subreddit'].map(label_dict)\n",
    "\n",
    "# Split the data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['combined_text'].values, df['label'].values, test_size=0.1)\n",
    "\n",
    "# Create the dataset\n",
    "train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = TextDataset(val_texts, val_labels, tokenizer)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=len(label_dict)  # Number of output labels\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gavingalusha/anaconda3/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 15\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "Cell \u001b[0;32mIn[19], line 15\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m---> 15\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     16\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbatch)\n\u001b[1;32m     17\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Evaluation step could be added here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Loss: 1.2010806798934937\n",
      "Current Loss: 1.1881211996078491\n",
      "Current Loss: 1.2479181289672852\n",
      "Current Loss: 1.1865103244781494\n",
      "Current Loss: 1.1670149564743042\n",
      "Current Loss: 1.0979158878326416\n",
      "Current Loss: 1.1490139961242676\n",
      "Current Loss: 1.124993085861206\n",
      "Current Loss: 1.0458067655563354\n",
      "Current Loss: 1.129644513130188\n",
      "Current Loss: 1.0861319303512573\n",
      "Current Loss: 1.1049641370773315\n",
      "Current Loss: 1.172973394393921\n",
      "Current Loss: 1.0778130292892456\n",
      "Current Loss: 1.085856318473816\n",
      "Current Loss: 1.112165927886963\n",
      "Current Loss: 1.085579752922058\n",
      "Current Loss: 1.1484427452087402\n",
      "Current Loss: 1.1397459506988525\n",
      "Current Loss: 1.0893964767456055\n",
      "Current Loss: 1.087742567062378\n",
      "Current Loss: 1.109257698059082\n",
      "Current Loss: 1.0647058486938477\n",
      "Current Loss: 1.058986783027649\n",
      "Current Loss: 1.124523401260376\n",
      "Current Loss: 1.0965850353240967\n",
      "Current Loss: 1.044784426689148\n",
      "Current Loss: 1.0558671951293945\n",
      "Current Loss: 1.0312113761901855\n",
      "Current Loss: 1.0537627935409546\n",
      "Current Loss: 1.0970659255981445\n",
      "Current Loss: 1.0519256591796875\n",
      "Current Loss: 1.0571330785751343\n",
      "Current Loss: 0.9968277215957642\n",
      "Current Loss: 1.0430783033370972\n",
      "Current Loss: 1.0999038219451904\n",
      "Current Loss: 1.0028735399246216\n",
      "Current Loss: 1.0148545503616333\n",
      "Current Loss: 1.0092562437057495\n",
      "Current Loss: 0.9231747984886169\n",
      "Current Loss: 0.9358212947845459\n",
      "Current Loss: 0.9267780184745789\n",
      "Current Loss: 0.9191327691078186\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.train()\n",
    "for batch in train_loader:\n",
    "    # Ensure batch is a dictionary\n",
    "    if not isinstance(batch, dict):\n",
    "        raise ValueError(\"Expected batch to be a dictionary.\")\n",
    "\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "\n",
    "    # Clear any previously calculated gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    # Backward pass and optimize\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Current Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
